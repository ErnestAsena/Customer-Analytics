{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Conversion Prediction with Deep Learning, TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "We have data from an Audiobook app. Logically, it relates only to the audio versions of books. Each customer in the database has made a purchase at least once, that's why he/she is in the database. We want to create a machine learning algorithm based on our available data that can predict if a customer will buy again from the Audiobook company.\n",
    "\n",
    "The main idea is that if a customer has a low probability of coming back, there is no reason to spend any money on advertizing to him/her. If we can focus our efforts ONLY on customers that are likely to convert again, we can make great savings. \n",
    "\n",
    "There are several variables: Customer ID, Book length in mins_avg (average of all purchases), Book length in minutes_sum (sum of all purchases), Price Paid_avg (average of all purchases), Price paid_sum (sum of all purchases), Review (a Boolean variable), Review (out of 10), Total minutes listened, Completion (from 0 to 1), Support requests (number), and Last visited minus purchase date (in days).\n",
    "\n",
    "So these are the inputs (excluding customer ID, as it is completely arbitrary).\n",
    "\n",
    "The targets are a Boolean variable (so 0, or 1). We are taking a period of 2 years in our inputs, and the next 6 months as targets. So, in fact, we are predicting if: based on the last 2 years of activity and engagement, a customer will convert in the next 6 months. 6 months sounds like a reasonable time. If they don't convert after 6 months, chances are they've gone to a competitor or didn't like the Audiobook way of digesting information. \n",
    "\n",
    "The task is to create a machine learning algorithm, which is able to predict if a customer will buy again. \n",
    "\n",
    "This is a classification problem with two classes: won't buy and will buy, represented by 0s and 1s. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')\n",
    "\n",
    "# The inputs are all columns in the csv, except for the first one and the last one\n",
    "# The first column is the arbitrary ID, while the last contains the targets\n",
    "\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "\n",
    "# The targets are in the last column. \n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many targets are 1 (meaning that the customer did convert)\n",
    "num_one_targets = int(np.sum(targets_all))\n",
    "\n",
    "# Set a counter for targets that are 0 (meaning that the customer did not convert)\n",
    "zero_targets_counter = 0\n",
    "\n",
    "# We want to create a \"balanced\" dataset, so we will have to remove some input/target pairs.\n",
    "# Declare a variable that will do that:\n",
    "indices_to_remove = []\n",
    "\n",
    "# Count the number of targets that are 0. \n",
    "# Once there are as many 0s as 1s, mark entries where the target is 0.\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "# Create two new variables, one that will contain the inputs, and one that will contain the targets.\n",
    "# We delete all indices that we marked \"to remove\" in the loop above.\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crete a standar scaler object\n",
    "scaler_deep_learning = StandardScaler()\n",
    "# Fit and transform the original data\n",
    "scaled_inputs = scaler_deep_learning.fit_transform(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the data was collected it was actually arranged by date\n",
    "# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.\n",
    "# Since we will be batching, we want the data to be as randomly spread out as possible\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Use the shuffled indices to shuffle the inputs and targets.\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1798.0 3579 0.502374965074043\n",
      "218.0 447 0.48769574944071586\n",
      "221.0 448 0.49330357142857145\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of samples\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Count the samples in each subset, assuming we want 80-10-10 distribution of training, validation, and test.\n",
    "# Naturally, the numbers are integers.\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "\n",
    "# The 'test' dataset contains all remaining data.\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "# Create variables that record the inputs and targets for training\n",
    "# In our shuffled dataset, they are the first \"train_samples_count\" observations\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for validation.\n",
    "# They are the next \"validation_samples_count\" observations, folllowing the \"train_samples_count\" we already assigned\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for test.\n",
    "# They are everything that is remaining.\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "\n",
    "\n",
    "# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the three datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the three datasets in *.npz.\n",
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(scaler_deep_learning, open('scaler_deep_learning.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the machine learning algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a temporary variable npz, where we will store each of the three Audiobooks datasets\n",
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "\n",
    "# we extract the inputs using the keyword under which we saved them\n",
    "train_inputs = npz['inputs'].astype(np.float)\n",
    "train_targets = npz['targets'].astype(np.int)\n",
    "\n",
    "# we load the validation data in the temporary variable\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "\n",
    "# we load the test data in the temporary variable\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "# create 2 variables that will contain the test inputs and the test targets\n",
    "test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Outline, optimizers, loss, early stopping and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-09 07:29:45.338899: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 [==============================] - 2s 8ms/step - loss: 0.5752 - accuracy: 0.7795 - val_loss: 0.4301 - val_accuracy: 0.8837\n",
      "Epoch 2/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3938 - accuracy: 0.8678 - val_loss: 0.3211 - val_accuracy: 0.8926\n",
      "Epoch 3/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3340 - accuracy: 0.8762 - val_loss: 0.2876 - val_accuracy: 0.8971\n",
      "Epoch 4/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.3081 - accuracy: 0.8852 - val_loss: 0.2700 - val_accuracy: 0.9038\n",
      "Epoch 5/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2919 - accuracy: 0.8916 - val_loss: 0.2583 - val_accuracy: 0.9060\n",
      "Epoch 6/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2795 - accuracy: 0.8947 - val_loss: 0.2526 - val_accuracy: 0.9128\n",
      "Epoch 7/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2710 - accuracy: 0.8958 - val_loss: 0.2447 - val_accuracy: 0.9105\n",
      "Epoch 8/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.8980 - val_loss: 0.2440 - val_accuracy: 0.9105\n",
      "Epoch 9/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2598 - accuracy: 0.9005 - val_loss: 0.2390 - val_accuracy: 0.9128\n",
      "Epoch 10/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2561 - accuracy: 0.9000 - val_loss: 0.2362 - val_accuracy: 0.9105\n",
      "Epoch 11/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2552 - accuracy: 0.9005 - val_loss: 0.2377 - val_accuracy: 0.9105\n",
      "Epoch 12/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2515 - accuracy: 0.9042 - val_loss: 0.2315 - val_accuracy: 0.9150\n",
      "Epoch 13/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2489 - accuracy: 0.9030 - val_loss: 0.2303 - val_accuracy: 0.9128\n",
      "Epoch 14/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2467 - accuracy: 0.9061 - val_loss: 0.2344 - val_accuracy: 0.9150\n",
      "Epoch 15/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2453 - accuracy: 0.9061 - val_loss: 0.2263 - val_accuracy: 0.9172\n",
      "Epoch 16/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2417 - accuracy: 0.9067 - val_loss: 0.2295 - val_accuracy: 0.9128\n",
      "Epoch 17/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2393 - accuracy: 0.9072 - val_loss: 0.2293 - val_accuracy: 0.9195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x161adf550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_size = 10\n",
    "# Set the output size\n",
    "output_size = 2\n",
    "hidden_layer_size = 50\n",
    "    \n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])\n",
    "\n",
    "\n",
    "### Choose the optimizer and the loss function\n",
    "\n",
    "# we define the optimizer we'd like to use, \n",
    "# the loss function, \n",
    "# and the metrics we are interested in obtaining at each iteration\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "### Training\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# set a maximum number of training epochs\n",
    "max_epochs = 100\n",
    "\n",
    "# set an early stopping mechanism\n",
    "# let's set patience=2, to be a bit tolerant against random validation loss increases\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "# fit the model\n",
    "model.fit(train_inputs, # train inputs\n",
    "          train_targets, # train targets\n",
    "          batch_size=batch_size, # batch size\n",
    "          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n",
    "          callbacks=[early_stopping], # early stopping\n",
    "          validation_data=(validation_inputs, validation_targets), # validation data\n",
    "          verbose = 1 \n",
    "          )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2091 - accuracy: 0.9330\n"
     ]
    }
   ],
   "source": [
    "# declare two variables that are going to contain the two outputs from the evaluate function\n",
    "# they are the loss (which is there by default) and whatever was specified in the 'metrics' argument when fitting the model\n",
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.21. Test accuracy: 93.30%\n"
     ]
    }
   ],
   "source": [
    "# Print the result in neatly formatted\n",
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the probability for a customer to convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.99, 0.01],\n",
       "       [0.02, 0.98],\n",
       "       [0.15, 0.85],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.98, 0.02],\n",
       "       [0.2 , 0.8 ],\n",
       "       [0.95, 0.05],\n",
       "       [0.31, 0.69],\n",
       "       [0.18, 0.82],\n",
       "       [0.97, 0.03],\n",
       "       [1.  , 0.  ],\n",
       "       [0.98, 0.02],\n",
       "       [0.06, 0.94],\n",
       "       [0.38, 0.62],\n",
       "       [0.83, 0.17],\n",
       "       [0.35, 0.65],\n",
       "       [0.08, 0.92],\n",
       "       [1.  , 0.  ],\n",
       "       [0.97, 0.03],\n",
       "       [0.2 , 0.8 ],\n",
       "       [0.92, 0.08],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.34, 0.66],\n",
       "       [0.66, 0.34],\n",
       "       [0.36, 0.64],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.88, 0.12],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.02, 0.98],\n",
       "       [0.  , 1.  ],\n",
       "       [0.89, 0.11],\n",
       "       [1.  , 0.  ],\n",
       "       [0.72, 0.28],\n",
       "       [0.24, 0.76],\n",
       "       [0.93, 0.07],\n",
       "       [0.93, 0.07],\n",
       "       [1.  , 0.  ],\n",
       "       [0.03, 0.97],\n",
       "       [0.96, 0.04],\n",
       "       [0.  , 1.  ],\n",
       "       [0.2 , 0.8 ],\n",
       "       [0.66, 0.34],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.96, 0.04],\n",
       "       [1.  , 0.  ],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.98, 0.02],\n",
       "       [0.88, 0.12],\n",
       "       [0.77, 0.23],\n",
       "       [0.23, 0.77],\n",
       "       [0.  , 1.  ],\n",
       "       [0.91, 0.09],\n",
       "       [0.  , 1.  ],\n",
       "       [0.94, 0.06],\n",
       "       [0.93, 0.07],\n",
       "       [0.39, 0.61],\n",
       "       [0.91, 0.09],\n",
       "       [0.94, 0.06],\n",
       "       [0.85, 0.15],\n",
       "       [0.72, 0.28],\n",
       "       [0.29, 0.71],\n",
       "       [0.95, 0.05],\n",
       "       [0.99, 0.01],\n",
       "       [0.24, 0.76],\n",
       "       [0.01, 0.99],\n",
       "       [0.82, 0.18],\n",
       "       [1.  , 0.  ],\n",
       "       [0.19, 0.81],\n",
       "       [0.98, 0.02],\n",
       "       [1.  , 0.  ],\n",
       "       [0.18, 0.82],\n",
       "       [1.  , 0.  ],\n",
       "       [0.17, 0.83],\n",
       "       [0.97, 0.03],\n",
       "       [0.1 , 0.9 ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.12, 0.88],\n",
       "       [0.98, 0.02],\n",
       "       [0.36, 0.64],\n",
       "       [0.17, 0.83],\n",
       "       [0.8 , 0.2 ],\n",
       "       [0.14, 0.86],\n",
       "       [0.  , 1.  ],\n",
       "       [0.78, 0.22],\n",
       "       [0.25, 0.75],\n",
       "       [0.01, 0.99],\n",
       "       [0.99, 0.01],\n",
       "       [0.06, 0.94],\n",
       "       [0.97, 0.03],\n",
       "       [0.96, 0.04],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.17, 0.83],\n",
       "       [0.16, 0.84],\n",
       "       [0.  , 1.  ],\n",
       "       [0.96, 0.04],\n",
       "       [0.23, 0.77],\n",
       "       [0.18, 0.82],\n",
       "       [0.94, 0.06],\n",
       "       [0.  , 1.  ],\n",
       "       [0.87, 0.13],\n",
       "       [0.  , 1.  ],\n",
       "       [0.1 , 0.9 ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.11, 0.89],\n",
       "       [0.  , 1.  ],\n",
       "       [0.87, 0.13],\n",
       "       [0.97, 0.03],\n",
       "       [1.  , 0.  ],\n",
       "       [0.97, 0.03],\n",
       "       [1.  , 0.  ],\n",
       "       [0.12, 0.88],\n",
       "       [0.77, 0.23],\n",
       "       [1.  , 0.  ],\n",
       "       [0.87, 0.13],\n",
       "       [0.73, 0.27],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.92, 0.08],\n",
       "       [0.18, 0.82],\n",
       "       [0.91, 0.09],\n",
       "       [0.94, 0.06],\n",
       "       [0.75, 0.25],\n",
       "       [0.91, 0.09],\n",
       "       [1.  , 0.  ],\n",
       "       [0.97, 0.03],\n",
       "       [0.94, 0.06],\n",
       "       [0.22, 0.78],\n",
       "       [0.17, 0.83],\n",
       "       [0.  , 1.  ],\n",
       "       [0.06, 0.94],\n",
       "       [0.88, 0.12],\n",
       "       [0.96, 0.04],\n",
       "       [0.19, 0.81],\n",
       "       [0.15, 0.85],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.15, 0.85],\n",
       "       [0.66, 0.34],\n",
       "       [0.03, 0.97],\n",
       "       [0.94, 0.06],\n",
       "       [0.21, 0.79],\n",
       "       [0.36, 0.64],\n",
       "       [0.96, 0.04],\n",
       "       [0.18, 0.82],\n",
       "       [0.35, 0.65],\n",
       "       [0.  , 1.  ],\n",
       "       [0.93, 0.07],\n",
       "       [1.  , 0.  ],\n",
       "       [0.09, 0.91],\n",
       "       [0.36, 0.64],\n",
       "       [0.09, 0.91],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.11, 0.89],\n",
       "       [0.97, 0.03],\n",
       "       [0.  , 1.  ],\n",
       "       [0.8 , 0.2 ],\n",
       "       [0.24, 0.76],\n",
       "       [0.  , 1.  ],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.18, 0.82],\n",
       "       [0.23, 0.77],\n",
       "       [0.95, 0.05],\n",
       "       [0.99, 0.01],\n",
       "       [0.15, 0.85],\n",
       "       [0.12, 0.88],\n",
       "       [0.93, 0.07],\n",
       "       [0.99, 0.01],\n",
       "       [0.89, 0.11],\n",
       "       [0.35, 0.65],\n",
       "       [0.26, 0.74],\n",
       "       [0.  , 1.  ],\n",
       "       [0.24, 0.76],\n",
       "       [0.48, 0.52],\n",
       "       [1.  , 0.  ],\n",
       "       [0.16, 0.84],\n",
       "       [0.95, 0.05],\n",
       "       [0.99, 0.01],\n",
       "       [1.  , 0.  ],\n",
       "       [0.67, 0.33],\n",
       "       [0.01, 0.99],\n",
       "       [0.76, 0.24],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.12, 0.88],\n",
       "       [0.97, 0.03],\n",
       "       [0.06, 0.94],\n",
       "       [0.  , 1.  ],\n",
       "       [0.04, 0.96],\n",
       "       [1.  , 0.  ],\n",
       "       [0.87, 0.13],\n",
       "       [0.1 , 0.9 ],\n",
       "       [0.75, 0.25],\n",
       "       [1.  , 0.  ],\n",
       "       [0.89, 0.11],\n",
       "       [0.92, 0.08],\n",
       "       [1.  , 0.  ],\n",
       "       [0.01, 0.99],\n",
       "       [0.  , 1.  ],\n",
       "       [0.13, 0.87],\n",
       "       [0.87, 0.13],\n",
       "       [0.36, 0.64],\n",
       "       [0.96, 0.04],\n",
       "       [0.69, 0.31],\n",
       "       [0.14, 0.86],\n",
       "       [0.93, 0.07],\n",
       "       [0.65, 0.35],\n",
       "       [0.97, 0.03],\n",
       "       [0.91, 0.09],\n",
       "       [0.94, 0.06],\n",
       "       [0.36, 0.64],\n",
       "       [0.36, 0.64],\n",
       "       [0.92, 0.08],\n",
       "       [0.74, 0.26],\n",
       "       [0.1 , 0.9 ],\n",
       "       [0.85, 0.15],\n",
       "       [0.  , 1.  ],\n",
       "       [0.22, 0.78],\n",
       "       [0.91, 0.09],\n",
       "       [0.61, 0.39],\n",
       "       [0.22, 0.78],\n",
       "       [0.13, 0.87],\n",
       "       [0.96, 0.04],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.34, 0.66],\n",
       "       [0.  , 1.  ],\n",
       "       [0.71, 0.29],\n",
       "       [0.94, 0.06],\n",
       "       [0.88, 0.12],\n",
       "       [0.13, 0.87],\n",
       "       [1.  , 0.  ],\n",
       "       [0.95, 0.05],\n",
       "       [0.17, 0.83],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.31, 0.69],\n",
       "       [0.97, 0.03],\n",
       "       [0.23, 0.77],\n",
       "       [1.  , 0.  ],\n",
       "       [0.88, 0.12],\n",
       "       [0.13, 0.87],\n",
       "       [1.  , 0.  ],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.71, 0.29],\n",
       "       [0.49, 0.51],\n",
       "       [1.  , 0.  ],\n",
       "       [0.78, 0.22],\n",
       "       [0.  , 1.  ],\n",
       "       [0.59, 0.41],\n",
       "       [0.15, 0.85],\n",
       "       [0.36, 0.64],\n",
       "       [0.65, 0.35],\n",
       "       [0.  , 1.  ],\n",
       "       [0.24, 0.76],\n",
       "       [0.74, 0.26],\n",
       "       [1.  , 0.  ],\n",
       "       [0.18, 0.82],\n",
       "       [0.11, 0.89],\n",
       "       [0.16, 0.84],\n",
       "       [0.18, 0.82],\n",
       "       [0.97, 0.03],\n",
       "       [0.  , 1.  ],\n",
       "       [0.36, 0.64],\n",
       "       [1.  , 0.  ],\n",
       "       [0.18, 0.82],\n",
       "       [0.01, 0.99],\n",
       "       [0.07, 0.93],\n",
       "       [0.76, 0.24],\n",
       "       [0.77, 0.23],\n",
       "       [0.94, 0.06],\n",
       "       [0.  , 1.  ],\n",
       "       [0.1 , 0.9 ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.97, 0.03],\n",
       "       [0.82, 0.18],\n",
       "       [0.91, 0.09],\n",
       "       [0.06, 0.94],\n",
       "       [1.  , 0.  ],\n",
       "       [0.84, 0.16],\n",
       "       [0.16, 0.84],\n",
       "       [0.18, 0.82],\n",
       "       [0.67, 0.33],\n",
       "       [1.  , 0.  ],\n",
       "       [0.19, 0.81],\n",
       "       [0.11, 0.89],\n",
       "       [0.09, 0.91],\n",
       "       [0.15, 0.85],\n",
       "       [0.36, 0.64],\n",
       "       [0.97, 0.03],\n",
       "       [0.36, 0.64],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.19, 0.81],\n",
       "       [0.1 , 0.9 ],\n",
       "       [0.1 , 0.9 ],\n",
       "       [0.18, 0.82],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.97, 0.03],\n",
       "       [0.97, 0.03],\n",
       "       [0.94, 0.06],\n",
       "       [0.88, 0.12],\n",
       "       [0.24, 0.76],\n",
       "       [0.14, 0.86],\n",
       "       [1.  , 0.  ],\n",
       "       [0.18, 0.82],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.32, 0.68],\n",
       "       [0.87, 0.13],\n",
       "       [0.  , 1.  ],\n",
       "       [0.23, 0.77],\n",
       "       [1.  , 0.  ],\n",
       "       [0.88, 0.12],\n",
       "       [0.  , 1.  ],\n",
       "       [0.97, 0.03],\n",
       "       [1.  , 0.  ],\n",
       "       [0.18, 0.82],\n",
       "       [0.97, 0.03],\n",
       "       [1.  , 0.  ],\n",
       "       [0.31, 0.69],\n",
       "       [0.12, 0.88],\n",
       "       [0.03, 0.97],\n",
       "       [0.  , 1.  ],\n",
       "       [0.61, 0.39],\n",
       "       [0.1 , 0.9 ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.95, 0.05],\n",
       "       [0.14, 0.86],\n",
       "       [1.  , 0.  ],\n",
       "       [0.01, 0.99],\n",
       "       [0.  , 1.  ],\n",
       "       [0.79, 0.21],\n",
       "       [0.24, 0.76],\n",
       "       [0.19, 0.81],\n",
       "       [0.04, 0.96],\n",
       "       [0.66, 0.34],\n",
       "       [0.72, 0.28],\n",
       "       [0.  , 1.  ],\n",
       "       [0.91, 0.09],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.09, 0.91],\n",
       "       [0.  , 1.  ],\n",
       "       [0.01, 0.99],\n",
       "       [0.  , 1.  ],\n",
       "       [0.96, 0.04],\n",
       "       [0.98, 0.02],\n",
       "       [0.  , 1.  ],\n",
       "       [0.01, 0.99],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.21, 0.79],\n",
       "       [0.  , 1.  ],\n",
       "       [0.22, 0.78],\n",
       "       [0.86, 0.14],\n",
       "       [0.12, 0.88],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.14, 0.86],\n",
       "       [0.98, 0.02],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.23, 0.77],\n",
       "       [0.02, 0.98],\n",
       "       [0.4 , 0.6 ],\n",
       "       [0.25, 0.75],\n",
       "       [0.99, 0.01],\n",
       "       [1.  , 0.  ],\n",
       "       [0.84, 0.16],\n",
       "       [0.87, 0.13],\n",
       "       [0.07, 0.93],\n",
       "       [0.  , 1.  ],\n",
       "       [0.01, 0.99],\n",
       "       [0.72, 0.28],\n",
       "       [0.85, 0.15],\n",
       "       [0.15, 0.85],\n",
       "       [0.01, 0.99],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.94, 0.06],\n",
       "       [0.2 , 0.8 ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.15, 0.85],\n",
       "       [0.93, 0.07],\n",
       "       [0.94, 0.06],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.95, 0.05],\n",
       "       [0.94, 0.06],\n",
       "       [0.94, 0.06],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.18, 0.82],\n",
       "       [0.2 , 0.8 ],\n",
       "       [0.95, 0.05],\n",
       "       [1.  , 0.  ],\n",
       "       [0.84, 0.16],\n",
       "       [0.14, 0.86],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.11, 0.89],\n",
       "       [0.78, 0.22],\n",
       "       [0.  , 1.  ],\n",
       "       [0.81, 0.19],\n",
       "       [0.  , 1.  ],\n",
       "       [0.22, 0.78],\n",
       "       [0.  , 1.  ],\n",
       "       [0.85, 0.15],\n",
       "       [0.99, 0.01],\n",
       "       [0.35, 0.65],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.15, 0.85],\n",
       "       [1.  , 0.  ],\n",
       "       [0.09, 0.91],\n",
       "       [0.  , 1.  ],\n",
       "       [0.92, 0.08],\n",
       "       [0.24, 0.76],\n",
       "       [0.31, 0.69],\n",
       "       [0.18, 0.82],\n",
       "       [0.18, 0.82],\n",
       "       [0.85, 0.15],\n",
       "       [0.36, 0.64],\n",
       "       [0.13, 0.87],\n",
       "       [0.  , 1.  ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can predict the probability of each class using the 'predict' method\n",
    "model.predict(test_inputs).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
       "       1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively, we can get only the second column\n",
    "# The main idea is that we are often interested in ONLY ONE of the two outcomes\n",
    "# In this case we would like to know if the customer will convert again\n",
    "model.predict(test_inputs)[:,1].round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A much better approach here would be to use argmax (arguments of the maxima)\n",
    "# Argmax indicates the position of the highest argument row-wise or column-wise\n",
    "# In our case, we want ot know which COLUMN has the higher argument (probability), therefore we set axis=1 (for columns)\n",
    "# The output would be the column ID with the highest argument for each observation (row)\n",
    "# For instance, the first observation (in our output) was [0.93,0.07]\n",
    "# np.argmax([0.93,0.07], axis=1) would find that 0.91 is the higher argument (higher probability) and return 0\n",
    "# This method is great for multi-class problems as it is independent of the number of classes\n",
    "np.argmax(model.predict(test_inputs),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we save the model using the built-in method TensorFlow method\n",
    "# We choose the name and the file extension\n",
    "# Since the HDF format is optimal for large numerical objects, that's our preferred choice here (and the one recommended by TF)\n",
    "# The proper extension is .h5 to indicate HDF, version 5\n",
    "model.save('audiobooks_model.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the scaler and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We load the scaler we had saved earlier using the pickle method load\n",
    "scaler_deep_learning = pickle.load(open('scaler_deep_learning.pickle', 'rb'))\n",
    "# To load the model, we use the TensorFlow (Keras) function relevant for the operation\n",
    "model = tf.keras.models.load_model('audiobooks_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new data is located in 'New_Audiobooks_Data.csv'\n",
    "raw_data = np.loadtxt('New_Audiobooks_Data.csv',delimiter=',')\n",
    "# We are interested in all data except for the first column (ID)\n",
    "new_data_inputs = raw_data[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the probability of a customer to convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the new data in the same way we scaled the train data\n",
    "new_data_inputs_scaled = scaler_deep_learning.transform(new_data_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.  , 0.04, 1.  , 0.  , 0.03, 0.03, 0.03, 0.03, 0.72, 0.  ,\n",
       "       0.77, 0.91, 0.  , 0.12, 0.15, 0.9 , 0.65, 0.86, 0.95, 1.  , 1.  ,\n",
       "       1.  , 0.  , 0.  , 0.95, 0.29, 0.  , 1.  , 1.  ], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the probability of each customer to convert\n",
    "# Here we have also taken only the second column and rounded to 2 digits after the dot\n",
    "model.predict(new_data_inputs_scaled)[:,1].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement the better approach which is independent of the number of classes\n",
    "np.argmax(model.predict(new_data_inputs_scaled),1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
